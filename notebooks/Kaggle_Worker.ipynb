{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan 2.2 Worker Node (Kaggle Edition)\n",
    "\n",
    "Run this notebook on Kaggle (GPU T4 x2 or P100) to join the distributed pool.\n",
    "\n",
    "**Features:**\n",
    "- **Auto-Scaling:** Automatically launches 2 workers if 2 GPUs are detected (for 1.3B model).\n",
    "- **Safety:** Limits to 1 worker for the 14B model to prevent System RAM OOM.\n",
    "\n",
    "**Pre-requisites:**\n",
    "1.  You must have the Controller running (e.g., on a VPS or via Ngrok).\n",
    "2.  Copy your Controller URL (e.g., `wss://my-controller.ngrok.io/ws/worker`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!pip install -q diffusers transformers accelerate sentencepiece websockets pydantic fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Clone the Codebase\n",
    "!git clone https://github.com/your-repo/wan_pool.git\n",
    "%cd wan_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configure & Run Worker(s)\n",
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "import time\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CONTROLLER_URL = \"wss://your-controller-url.com/ws/worker\"  # <--- PASTE YOUR URL HERE\n",
    "MODEL_ID = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\" # Or \"Wan-AI/Wan2.1-T2V-14B-Diffusers\"\n",
    "# ---------------------\n",
    "\n",
    "def launch_workers():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Detected {gpu_count} GPUs.\")\n",
    "    \n",
    "    # Decision Logic\n",
    "    num_workers = gpu_count\n",
    "    if \"14B\" in MODEL_ID and gpu_count > 1:\n",
    "        print(\"WARNING: 14B Model detected. Restricting to 1 Worker to prevent System RAM OOM.\")\n",
    "        num_workers = 1\n",
    "    \n",
    "    processes = []\n",
    "    try:\n",
    "        for i in range(num_workers):\n",
    "            print(f\"Launching Worker {i+1}/{num_workers} on GPU {i}...\")\n",
    "            env = os.environ.copy()\n",
    "            env[\"CONTROLLER_URL\"] = CONTROLLER_URL\n",
    "            env[\"MODEL_ID\"] = MODEL_ID\n",
    "            env[\"CUDA_VISIBLE_DEVICES\"] = str(i)\n",
    "            env[\"WORKER_NAME\"] = f\"Kaggle-Worker-GPU{i}\"\n",
    "            \n",
    "            # Start process\n",
    "            p = subprocess.Popen(\n",
    "                [sys.executable, \"-m\", \"src.worker_client\"],\n",
    "                env=env\n",
    "            )\n",
    "            processes.append(p)\n",
    "        \n",
    "        print(f\"Running {len(processes)} workers. Press Stop to exit.\")\n",
    "        # Wait for processes\n",
    "        for p in processes:\n",
    "            p.wait()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping workers...\")\n",
    "        for p in processes:\n",
    "            p.terminate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    launch_workers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
